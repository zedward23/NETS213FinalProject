# -*- coding: utf-8 -*-
"""NETS213 Sentiment Analysis2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xhuyxVku5HhygxGZcBse2gMVDAQ9R2SK

# Text Classification

Text classification is one of the tasks that is addressed in natural language processing (NLP).  Like with computer vision, NLP uses deep learning.  A particular kind of deep learning model that is used in NLP is called the transformer.  If you're interested in learning  about transformers in this [blog post](http://jalammar.github.io/illustrated-transformer/).  We'll be using an implementation of transformers from an open source package called Hugging Face.  

For this assignment, we'll look at wallk through a text classification task called *intent detection*.  When you talk to your Amazon Alexa, it needs to figure out what you're trying to do.  Are you trying to play music?  Do you want to check the weather?  Are you setting a timer?  Are you trying to get a recipe to cook something?  Depending on what it thinks your intent is, it routes your message to a specialized module to handle your request.


### Let's get started!

Run the following cells to download the dataset and preprocess it.
"""

#@title Run me and restart runtime environment!
###################### Restart Runtime after executing this cell ######################################################
!pip install simpletransformers

#@title Run me! ‚Äì Data Preprocessing 1 
###**Do not modify the code in the cells below (I am planning to remove some of the below cells used for preperocessing and subsettng the data and provide the final csv to the students instead)**
### gathering the dataset
from urllib.request import urlretrieve
from pathlib import Path
import pandas as pd

SNIPS_DATA_BASE_URL = (
    "https://github.com/ogrisel/slot_filling_and_intent_detection_of_SLU/blob/"
    "master/data/snips/"
)
for filename in ["train", "valid", "vocab.intent"]:
    path = Path(filename)
    if not path.exists():
        print(f"Downloading {filename}...")
        urlretrieve(SNIPS_DATA_BASE_URL + filename + "?raw=true", path)

#@title Run me! ‚Äì Data Preprocessing 2
### gathering the dataset
def parse_line(line):
    utterance_data, intent_label = line.split(" <=> ")
    items = utterance_data.split()
    words = [item.rsplit(":", 1)[0]for item in items]
    word_labels = [item.rsplit(":", 1)[1]for item in items]
    return {
        "intent_label": intent_label,
        "words": " ".join(words),
        "word_labels": " ".join(word_labels),
        "length": len(words),
    }

"""## Intent Classification Dataset 

In this part of the assignment, we will be using a subset of [intent classifcation dataset 
](https://github.com/ogrisel/slot_filling_and_intent_detection_of_SLU/tree/master/data/snips). We will be classifying a given sentence into one of the two categories:
AddToPlaylist and 
PlayMusic 

We will also provided a separate validation set to test the performance of the model. In the next cells, the data will be cleaned for you to use.

"""

#@title Run me! ‚Äì Data Cleaning & First 5 Rows of Dataset
################ data cleaning #############



# intent_names = Path("vocab.intent").read_text().split()
# intent_names=intent_names[0:1]+intent_names[3:4]
# intent_map = dict((label, idx) for idx, label in enumerate(intent_names))

# lines_train = Path("train").read_text().strip().splitlines()

# parsed = [parse_line(line) for line in lines_train]

# df_train = pd.DataFrame([p for p in parsed if p is not None])

# ## getting a subset of data
# count=500 
# #df_train_small= pd.concat([df_train[0:0+count],df_train[1842:1842+count],df_train[3715:3715+count],df_train[5615:5615+count],df_train[7515:7515+count],df_train[9371:9371+count],df_train[11225:11225+count]]) 

# df_train_= pd.concat([df_train[0:0+count],df_train[5615:5615+count]]) 

# df_train_=df_train_[['words','intent_label']]

# df_train_.replace({"intent_label":intent_map },inplace=True)

# df_train_=df_train_.sample(frac=1,random_state=56132).reset_index(drop=True)

# df_train_.head()


# ## introducing noise
# df_1=df_train_.sample(frac=0.3,random_state=111)

# df_0=df_train_.sample(frac=0.3,random_state=80)

# df_train_.loc[df_0.index ,'intent_label'] = 0
# df_train_.loc[df_1.index ,'intent_label'] = 1


# df_train_['intent_label'].value_counts()


df_train_ = pd.DataFrame(columns = ['words','intent_label'])

###### TEST DATA 1 #######
# df_train_['words'] = ['i did the math and [stockName] can murder at most another 36 children before it starts to affect the price. 36 deaths are basically priced in.',
#                       '[stockName] is like that homie that says theyre gonna hang out the next day but never does',
#                       '[stockName] 420 by 4/20 of course',
#                       'be grateful you lost all your money on options. there are people in third world countries who have no money to buy 0dte [stockName] puts.',
#                       'espn just posted that every nba team will be retiring the number 23 in honor of [stockName]. crazy.',
#                       '[stockName] is gonna hit 150 eow no more üçé‚Äòs only üçè‚Äôs this week',
#                       '[stockName] $150 by end of june',
#                       'if [stockName] doesn‚Äôt touch $420 by 04/20, i‚Äôm buying puts']
# df_train_['intent_label'] = [0,1,0,1,1,0,1,1]


##### TEST DATA 2 #######
df_train_ = pd.read_csv('bananorange.csv')

with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also
  display(df_train_)

"""Awesome, we're all set to get started with the actual analysis! In the next step, we will split the training and test data using the sklearn train_test_split function (see how it works [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). To decrease the training time (our current dataset has 1000 rows!), we will create a smaller subset of the training data called df_train_small. Note here that the parameter 'train_size' and 'test_size' indicate the proportion of data that is used for the training/testing dataset respectively. More information can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).

"""

from sklearn.model_selection import train_test_split  

def subset_datafarme(train_size):       
        X=df_train_['words']
        y=df_train_['intent_label']
        X_train, X_test, y_train, y_test = train_test_split( X, y, train_size=train_size, stratify=y,random_state=444)
        df_train_small=pd.DataFrame({'words':X_train,'intent_label':y_train})

        return df_train_small

"""By running the following cell, you can see the intent map, i.e. which intent is mapped to which label:"""

intent_map

#@title Run me! ‚Äì Validation Set Cleaning & First 5 Rows of Dataset

### cleaning validation set.
lines_valid = Path("valid").read_text().strip().splitlines()
df_valid = pd.DataFrame([parse_line(line) for line in lines_valid])
count=50
df_valid= pd.concat([df_valid[0:0+count],df_valid[300:300+count]]) 
df_valid=df_valid.sample(frac=1,random_state=1234).reset_index(drop=True)
df_valid=df_valid[['words','intent_label']]
df_valid.replace({"intent_label":intent_map },inplace=True)
df_valid.head()

"""## Defining the transformer model 
We will be using [Simple Transformers](https://github.com/ThilinaRajapakse/simpletransformers) to train our model. They are a wrapper library based on the [Transformer](https://github.com/huggingface/transformers) library of Hugging face. It let's you solve many interesting NLP problems with a few lines of code. Initially, we define the model. Here we've used the 'Bert Model' for training our classifier, but you can also try it with other [pre-trained models](https://huggingface.co/transformers/pretrained_models.html).
"""

from simpletransformers.classification import ClassificationModel, ClassificationArgs
import logging

logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.WARNING)

model_args = ClassificationArgs(num_train_epochs=1, overwrite_output_dir=True, manual_seed=42)
model = ClassificationModel(model_type='bert', model_name='bert-base-uncased', use_cuda=False, num_labels=2, args=model_args)

"""After it has been us talking for some time, it's your turn! Write a function "get_labels" that takes as input a prediction and returns the corresponding intent from the intent map that you saw above. This essentially means that if the input to the get_labels function is 0, it should return "AddToPlaylist" and if it is 1, it should return "PlayMusic""""

## TO DO: Write the get_labels function that converts the numerical predictions (0 or 1) to the actual labels (AddToPlaylist or PlayMusic)
## You can either hard code the solution or ‚Äì for a bit more challenging and in real-life more expandable implementation ‚Äì try to use the mapping that is stored in the intent_map that we saw earlier!

def get_labels(predictions):
    if predictions == 0:
      return 'Negative'
    elif predictions == 1:
      return 'Positive'

"""Good job! Now that we have that, we'll train our model. To ensure that training won't take too long, we set the training dataset size to be 0.1. There is a gap (indicated by the three dots) in the code for you to fill out though before we can continue :)"""

train_size=0.1
# df_train_small= subset_datafarme(train_size)
# df_train_small.shape
# display(df_train_small)

## Training the model
## TO DO: Add the missing input parameter
model.train_model(df_train_)

## Evaluate model
# result, model_outputs, wrong_predictions = model.eval_model(df_valid)

## Print the loss

# print(result['eval_loss'])

"""Now we finally get to the fun part ‚Äì using our model to predict the intent of a sentence! Again, we have left some gaps for you to fill:"""

## These are the sentences we make predictions for like "Play forties music on Pandora" or "Play the song domino by Luca Turilli". 
## TO DO: Add two relevant prediction sentences like the ones above in string format to the prediction_sentence array (where the three dots are)

prediction_sentences=["hate",
                      "poopoo"]

## TO DO: Fill out the missing input parameter for the predict function and see whether you can actually get predictions for your
## defined sentences!

predictions, raw_outputs = model.predict(prediction_sentences)

## With the following lines, we'll translate the predictions back to the intent using the get_labels function you designed earlier
for i in range(len(prediction_sentences)):
    print(prediction_sentences[i],":",get_labels(predictions[i]))

## make guesses for each comment in unlabled csv then output labeled csv

unlabeled_comments = pd.read_csv('bananorangeUnlabeled.csv')
# print(unlabeled_comments['words'].tolist())
# print(model.predict(unlabeled_comments['words'].tolist())[0])
unlabeled_comments['intent_label'] = model.predict(unlabeled_comments['words'].tolist())[0]
# display(unlabeled_comments)

labeled_comments = unlabeled_comments

labeled_comments.to_csv('labeled_comments1.csv', index = False)

"""That's another step done and dusted :) Now it's time to see how our model is doing overall by examining the accuracy scores. First, we will define a function to obtain the accuracy of our model:"""

from sklearn.metrics import accuracy_score

def accuracy_model(model,df_train_small):
    model.train_model(df_train_small)
   #result, model_outputs, wrong_predictions = model.eval_model(df_valid)
    predictions, raw_outputs = model.predict(df_valid['words'])
    return  accuracy_score(df_valid['intent_label'],predictions)

"""Next, we'll train the model with different training data sizes and save both the accuracy obtained (saved in accuracy_plot) and the size of the dataset (saved in train_shape) so that we can plot it later. Try it out with a couple of training sizes yourself ‚Äì add four to five values between 0 and 1 to the training size array and see how your plot in the next cell changes!"""

## TO DO: Add different training sizes between 0 and 1
train_sizes=[0.2, 0.4, 0.6, 0.8, 0.9]

train_shape=[]
accuracy_plot=[]
for train_size in train_sizes:    
    df_train_small=subset_datafarme(train_size)    
    acc=accuracy_model(model,df_train_small)
    print(train_size,len(df_train_small),acc)
    accuracy_plot.append(acc)
    train_shape.append(len(df_train_small))

"""And lastly, we want to plot the training size against the accuracy obtained. You know the game, fill out the input parameters to plot the size of the dataset on the x-axis and the accuracy on the y-axis:"""

import matplotlib.pyplot as plt

## TO DO: Add the missing input parameters
plt.plot(train_sizes, accuracy_plot)
plt.xlabel("size of dataset")
plt.ylabel("accuracy obatined")
plt.show()

"""Good job, you're done with this part of the assignment!"""